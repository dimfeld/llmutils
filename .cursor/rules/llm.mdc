---
description: Rules for using LLMs
alwaysApply: true
type: rules
---

We have the Vercel AI SDK, but for many LLM operations you want the user to be able to see what is streamed back. For such operations use `runStreamingPrmopt` instead. This function takes similar arguments to `generateText` or `streamText` but logs everything to the console and log file.

Example:

```
const result = await runStreamingPrompt({
  model,
  messages: [
    {
      role: 'user',
      content: fullPrompt,
    },
  ],
  temperature: 0.2,
});

let text = result.text;
```
