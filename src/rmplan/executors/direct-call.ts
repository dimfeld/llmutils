import { z } from 'zod';
import type { AgentCommandSharedOptions, Executor, ExecutorFactory } from './types';
import { DEFAULT_RUN_MODEL, runStreamingPrompt } from '../../common/run_and_apply';
import { applyLlmEdits, type ApplyLlmEditsOptions } from '../../apply-llm-edits/apply';
import { log } from '../../logging';
import { createRetryRequester } from '../../apply-llm-edits/retry.ts';
import type { RmplanConfig } from '../configSchema.ts';
import { getGitRoot } from '../../rmfilter/utils.ts';
import type { PrepareNextStepOptions } from '../actions.ts';

/**
 * Schema for the 'direct-call' executor's options.
 * It expects a model string for the LLM.
 */
const directCallOptionsSchema = z.object({
  executionModel: z
    .string()
    .describe("The model string for LLM execution, e.g., 'google/gemini-2.5-pro-preview-05-23'.")
    .optional(),
});

export type DirectCallExecutorOptions = z.infer<typeof directCallOptionsSchema>;

/**
 * The 'direct-call' executor.
 * This executor generates context using `rmfilter` and then directly calls an LLM
 * with that context. The LLM's response is then processed by `applyLlmEdits`.
 */
export class DirectCallExecutor implements Executor {
  static name = 'direct-call';
  static description =
    'Executes the plan by directly calling an LLM with context generated by rmfilter, then applies edits.';
  static optionsSchema = directCallOptionsSchema;

  constructor(
    public options: DirectCallExecutorOptions,
    public sharedOptions: AgentCommandSharedOptions,
    public rmplanConfig: RmplanConfig
  ) {}

  get executionModel() {
    return (
      this.rmplanConfig.models?.execution ??
      this.options.executionModel ??
      this.sharedOptions.model ??
      DEFAULT_RUN_MODEL
    );
  }

  prepareStepOptions() {
    const options: Partial<PrepareNextStepOptions> = {
      rmfilter: true,
      // improves caching
      rmfilterArgs: ['--omit-top-instructions'],
      model: this.executionModel,
    };

    return options;
  }

  async execute(contextContent: string) {
    const retryRequester = createRetryRequester(this.executionModel);
    const result = await runStreamingPrompt({
      input: contextContent,
      model: this.executionModel,
      temperature: 0,
      // Temperature and other LLM params could be configurable here or via rmplanConfig/executorOptions in future
    });

    const llmOutput = await result.text;

    // Add a newline for better separation in logs if streaming to console
    if (!process.stdout.isTTY) {
      // A simple check, or could be based on a quiet flag
      log('');
    }

    await applyLlmEdits({
      interactive: true,
      baseDir: await getGitRoot(this.sharedOptions.baseDir),
      content: llmOutput,
      retryRequester: retryRequester,
      originalPrompt: contextContent, // Pass the rmfilter output as the original prompt for retries
    });
  }
}
