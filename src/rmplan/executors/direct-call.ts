import { z } from 'zod';
import type { Executor } from './types';
import { DEFAULT_RUN_MODEL, runStreamingPrompt } from '../../common/run_and_apply';
import { applyLlmEdits } from '../../apply-llm-edits/apply';
import { log } from '../../logging';
import { createRetryRequester } from '../../apply-llm-edits/retry.ts';

/**
 * Schema for the 'direct-call' executor's options.
 * It expects a model string for the LLM.
 */
const directCallOptionsSchema = z.object({
  executionModel: z
    .string()
    .describe("The model string for LLM execution, e.g., 'google/gemini-2.5-pro-preview-05-23'.")
    .optional(),
});

export type DirectCallExecutorOptions = z.infer<typeof directCallOptionsSchema>;

/**
 * The 'direct-call' executor.
 * This executor generates context using `rmfilter` and then directly calls an LLM
 * with that context. The LLM's response is then processed by `applyLlmEdits`.
 */
export const directCallExecutor: Executor<typeof directCallOptionsSchema> = {
  name: 'direct-call',
  description:
    'Executes the plan by directly calling an LLM with context generated by rmfilter, then applies edits.',
  optionsSchema: directCallOptionsSchema,

  contextConfig: {
    runRmfilter: true, // This executor relies on rmfilter to generate the context
  },

  prepareStepOptions(executorOptions, sharedOptions, rmplanConfig) {
    const executionModel =
      rmplanConfig.models?.execution ??
      executorOptions.executionModel ??
      sharedOptions.model ??
      DEFAULT_RUN_MODEL;
    if (executionModel) {
      return {
        model: executionModel,
      };
    } else {
      return {};
    }
  },

  execute: async (
    contextContent, // This will be the output from rmfilter
    options,
    sharedOptions,
    rmplanConfig,
    baseApplyLlmEditsOptions
  ) => {
    const executionModel =
      rmplanConfig.models?.execution ??
      options.executionModel ??
      sharedOptions.model ??
      DEFAULT_RUN_MODEL;

    const retryRequester = createRetryRequester(executionModel);
    const result = await runStreamingPrompt({
      input: contextContent,
      model: executionModel,
      temperature: 0,
      // Temperature and other LLM params could be configurable here or via rmplanConfig/executorOptions in future
    });

    const llmOutput = await result.text;

    // Add a newline for better separation in logs if streaming to console
    if (!process.stdout.isTTY) {
      // A simple check, or could be based on a quiet flag
      log('');
    }

    await applyLlmEdits({
      content: llmOutput,
      retryRequester: retryRequester,
      ...baseApplyLlmEditsOptions,
      originalPrompt: contextContent, // Pass the rmfilter output as the original prompt for retries
    });
  },
};
