import { z } from 'zod';
import type { Executor } from './types';
import { runStreamingPrompt } from '../../common/run_and_apply';
import { applyLlmEdits } from '../../apply-llm-edits/apply';
import { log } from '../../logging';

/**
 * Schema for the 'direct-call' executor's options.
 * It expects a model string for the LLM.
 */
const directCallOptionsSchema = z
  .string()
  .describe("The model string for LLM execution, e.g., 'openai/gpt-4o'.");

export type DirectCallExecutorOptions = z.infer<typeof directCallOptionsSchema>;

/**
 * The 'direct-call' executor.
 * This executor generates context using `rmfilter` and then directly calls an LLM
 * with that context. The LLM's response is then processed by `applyLlmEdits`.
 */
export const directCallExecutor: Executor<typeof directCallOptionsSchema> = {
  name: 'direct-call',
  description:
    'Executes the plan by directly calling an LLM with context generated by rmfilter, then applies edits.',
  optionsSchema: directCallOptionsSchema,

  contextConfig: {
    runRmfilter: true, // This executor relies on rmfilter to generate the context
  },

  execute: async (
    contextContent, // This will be the output from rmfilter
    executionModel, // Parsed from optionsSchema (the model string)
    sharedOptions,
    rmplanConfig,
    retryRequester,
    baseApplyLlmEditsOptions
  ) => {
    // The 'contextContent' is the input for the LLM.
    // 'executionModel' is the specific model string for this executor.

    const result = await runStreamingPrompt({
      input: contextContent,
      model: executionModel,
      // Temperature and other LLM params could be configurable here or via rmplanConfig/executorOptions in future
    });

    const llmOutput = await result.text;

    // Add a newline for better separation in logs if streaming to console
    if (!process.stdout.isTTY) {
      // A simple check, or could be based on a quiet flag
      log('');
    }

    await applyLlmEdits({
      content: llmOutput,
      retryRequester: retryRequester,
      ...baseApplyLlmEditsOptions,
      originalPrompt: contextContent, // Pass the rmfilter output as the original prompt for retries
    });
  },
};
